////////////////////////////////////////////////////////////////////
// Copyright 2003 DEIS - Universita' di Bologna
// 
// name         simulator_statistics.txt
// author       DEIS - Universita' di Bologna
//              Davide Bertozzi - dbertozzi@deis.unibo.it
//              Mirko Loghi - mloghi@deis.unibo.it
//              Federico Angiolini - fangiolini@deis.unibo.it
//              Francesco Poletti - fpoletti@deis.unibo.it
// portions by  Massimo Scardamaglia - mascard@vizzavi.it
//              Francesco Menichelli - menichelli@mail.die.uniroma1.it
// info         Last revision: 27/09/2003
//
////////////////////////////////////////////////////////////////////


The platform has the ability to collect various statistics about the simulation
being performed. Please see simulator_switches.txt to see how to enable them.
Three main outputs can be generated:

1) stats.txt, with information on the number and location of performed
   accesses, bus usage, bus latencies, etc.
2) waves.vcd, with waveforms of the interconnection signals.
3) traceN.tra, with an access trace of every processor in the system.
   If DMA controllers are enabled, traces will be collected for them too.
   
These statistics, by default, will be written in the current directory. If you use
the automated cpl script without customization, however, they will be moved to the
$SWARMDIR after the end of the simulation.

-----------------------Statistics file

The stats.txt file is the main simulation output, and contains many useful statistics.
Usually you will want to have advanced statistics enabled (-s switch, see
simulator_switches.txt), since the impact on simulation performance is very small.
The file is composed of a few sections. We will now illustrate them in detail.

Input parameters summary
------------------------

This section summarizes the value of the command switches used to configure
the simulation run. It is useful both for debugging and for later reference.
The contents of this section should be self-explanatory.

Performance summary
-------------------

This section reports general statistics about simulation performance:
number of simulated cycles and overall simulation time. This also
allows computing an interesting metric about simulated CPU cycles/second.

Interconnection summary
-----------------------

This section provides some basic information about interconnection
performance. First of all, three numbers of simulated execution cycles
are reported. All of these three numbers refer to cycles spent inside
of start_metric()/stop_metric() triggers (please see the related section
below). The first one reports the number of cycles during which at least
one core was being profiled; the second one, the average number of cycles
spent within triggers by processors; the third one, the number of cycles
during which all of the system cores were collecting statistics. The last
figure is the one used for computing bus usage, because it is the only
one granting that traffic metrics are computed without processors
unexpectedly idle on the bus.
Following are two figures regarding bus traffic. "Bus busy" refers
to the amount of time during which there were outstanding interconnection
transactions, and is a metric of the amount of traffic generated by the
test benchmark. "Bus transferring data" is a subset of the previous entry,
and describes the amount of time spent actually moving data across the
interconnection. The ratio between this figure and the "Bus busy" one is
a very interesting metric of bus efficiency. (When analyzing efficiency of
the AMBA AHB bus, please take into account the pipelined nature of this
interconnection, which has the effect of severely lowering apparent
efficiency because of higher latencies).
Next, some statistics are reported about bus latencies. For both read and
write accesses, maximum, average and minimum latencies are presented. Two
sets of such results are shown: "Cycles to bus access" details latency
before getting bus ownership, "Cycles to bus completion" refers to the time
elapsed until the end of a full transaction. Example content:


Overall exec time          = 733941 cycles
1-CPU average exec time    = 682526 cycles
Concurrent exec time       = 601225 cycles
Bus busy                   = 196382 cycles (32.66% of 601225)
Bus transferring data      = 71384 cycles (11.87% of 601225, 36.35% of 196382)
Cycles to bus access (R)   = 39174 over 17988 accesses (max 15, avg 2.18, min 2)
Cycles to bus compl. (R)   = 77808 over 17988 accesses (max 22, avg 4.33, min 4)
Cycles to bus access (W)   = 134673 over 61024 accesses (max 13, avg 2.21, min 2)
Cycles to bus compl. (W)   = 256721 over 61024 accesses (max 15, avg 4.21, min 4)


Per-processor summary
---------------------

This section provides more details about performance of any single system core.
The stats.txt file will contain as many instances of this section as the
number of cores in the platform.
A first set of figures analyzes in depth the bus performance seen by the core.
Most numbers resemble what seen in the global bus performance section, but
for example single reads (i.e. the ones to non-cacheable devices, like the
shared and sempahore memories) are split from burst reads (i.e. cache refills
from private memory). Accesses to the DMA and scratchpad devices are computed
as a separate entry, because they make use of direct connections instead of
the global interconnect. An entry records the number of cycles spent not
actually in bus accesses, but inside of the SystemC wrapper; this number is an
overhead and is due to intrinsic features of our platform. The ""Free" bus
acceses" figure reports the number of transactions for which arbitration was
accomplished in the minimum possible time, because no other core was owning
the bus. Example content:

---------------
Arm Processor 0
---------------
Direct Accesses            = 0 to DMA
Bus Accesses               = 16010 (650 SR, 15251 SW, 109 BR)
Cycles to bus access (SR)  = 1441 over 650 accesses (max 9, avg 2.22, min 2)
Cycles to bus compl. (SR)  = 2741 over 650 accesses (max 11, avg 4.22, min 4)
Cycles to bus access (SW)  = 35123 over 15251 accesses (max 13, avg 2.30, min 2)
Cycles to bus compl. (SW)  = 65625 over 15251 accesses (max 15, avg 4.30, min 4)
Cycles to bus access (BR)  = 248 over 109 accesses (max 10, avg 2.28, min 2)
Cycles to bus compl. (BR)  = 1120 over 109 accesses (max 18, avg 10.28, min 10)
Cycles to bus access (tot) = 36812 over 16010 accesses (max 13, avg 2.30, min 2)
Cycles to bus compl. (tot) = 69486 over 16010 accesses (max 18, avg 4.34, min 4)
Wrapper overhead cycles    = 32020
Total bus activity cycles  = 101506 (bus completion + wrapper OH)
"Free" bus accesses        = 13161 (82.20% of 16010)


Per-processor access classification
-----------------------------------

This section is again processor-specific and may be present multiple times
in the stats.txt file. It is composed of one table, summarizing the
performance of the processor as currently configured (i.e. cache configuration,
presence of a scratchpad memory).
The table is composed of two columns. The left one describes actual
external accesses to a specific device, while the right one the number
of cycles internally spent by the core during such accesses. This amount is
additionally split in two figures: the number of cycles which would be
necessary with ideal, zero-WS peripherals, and the number of additional wait
cycles due to wrapping overhead, bus congestion and peripheral latencies.
The rows of the table match all possibile kind of device accesses. At the
bottom of the table, totals are computed. Please notice the "Pipeline
stalls" entry; this line detects detects cycles during which the core is
running without accessing any external data, and is due to instructions
which stall the core pipeline.
Example content:

+==================+=======================+
|                  |      Current setup    |
|                  |    Ext Acc     Cycles |
+==================+=======================+
| Private reads    |       109*     383874 |
| Bus+Wrapper waits|                   902 |
| Private writes   |      14601      14601 |
| Bus+Wrapper waits|                 77462 |
+==================+=======================+
| Scratch reads    |                     0 |
| Scratch writes   |                     0 |
+==================+=======================+
| Shared reads     |        640       1280 |
| Bus+Wrapper waits|                  3341 |
| Shared writes    |        640        640 |
| Bus+Wrapper waits|                  3363 |
+------------------+-----------------------+
| Semaphore reads  |         10         20 |
| Bus+Wrapper waits|                    50 |
| Semaphore writes |         10         10 |
| Bus+Wrapper waits|                    51 |
+------------------+-----------------------+
| Interrupt writes |          0          0 |
| Bus+Wrapper waits|                     0 |
+==================+=======================+
| Internal reads   |                     0 |
| Internal writes  |                     0 |
+==================+=======================+
| ARM total        |      16010     400425 |
| Wait cycles total|                 85169 |
| Pipeline stalls  |                115631 |
+------------------+-----------------------+
| Overall total    |      16010     601225 |
+==================+=======================+


Cache summary
-------------

This small section, on a per-processor basis, summarizes cache behaviour.
Statistics concerning hits and misses, for both the I-Cache and D-Cache,
are included. Please notice that D-Cache and I-Cache are separately tracked
even if a unified hardware cache is instantiated. Also, please remember
that the cache write policy is write-through.

DMA accesses
------------

This section is included in the stats.txt file if the DMA controllers are
enabled (see the -D switch in simulator_switches.txt). It provides some
statistics about the bus accesses performed by the DMA controllers.

-----------------------Access traces

These traces sequentially record all of the memory locations accessed by a core
during execution. If DMA is enabled, memory locations accessed by DMA controllers
will be recorded too. Such dumping will happen during the critical portions of
execution, as defined by triggers (see below). Traces span all memory addresses,
including private and shared memories, semaphores, other devices like DMA, scratchpad,
inter-processor interrupt controller, DMA, scratchpad. Only accesses to on-chip
built-in devices (e.g. the serial controller used to ouput benchmark messages) are
filtered away. A trace line takes the form:

00008614	1	0

This means that address 0x00008614 was accessed, and that the access was a read.
A write to the same address would have looked like

00008614	0	1

The second and third field of any entry can never have the same value. This
arrangement allows very easy post-processing to suit any need.

-----------------------VCD waveforms

In contrast to the other statistical outputs, VCD waveforms will always span the
whole length of the simulation. You can analyze them with standard, freely
available tools. If you have access to the source code of the simulator, you can
trim the amount of waveforms displayed (and thus hard disk space requirements) by
editing the $SWARMDIR/<architecture>/trace.cpp file tu suit your needs.

-----------------------Setting up benchmarks to trigger statistics

Advanced statistics (-s/-p switches, see simulator_switches.txt) and access
traces (-t switch, see simulator_switches.txt), if enabled, are collected during
configurable time spans. Two relevant choices can be made: collecting from
boot to some specific time (especially useful to monitor OS boot, by stopping
collection at the beginning of the execution of an OS-based testbench) or
during relevant sections of benchmark execution.

The simulator provides a simple way to choose what to do, by means of the -a
switch (see simulator_switches.txt). If -a is enabled, the simulator starts
collecting statistics immediately and waits for a stop signal.
If the variable is not set, the simulator waits for a start signal followed
by a stop signal.

It is then the testbench's responsibility to issue appropriate start/stop signals
to trigger statistics collection. Sample applications show how this can be done.
Basically, testbenches include the file "appsupport.h" provided with the
simulator; this file allows access to system calls which trigger simulation
messages. start_metric(), stop_metric() and stop_simulation() calls are defined.

If statistics collection is desired, start_metric() and stop_metric() calls
have to be issued both and in this order, else an error will be generated. The
only exception to this is when the -a switch is enabled, in which case one
start_metric() call is issued automatically. It is possible to resume statistics
collection after a stop_metric() call with a new start_metric(). This is especially
useful for incrementally monitoring execution inside of critical loops or functions.

The stop_simulation() call must follow a stop_metric() call, even though not
necessarily immediately; this call alerts the simulator that the current processor
has ended simulation activities and is ready to shutdown. Only when all of the
processors have issued a stop_simulation(), the simulation is actually stopped.
It is not possible to resume statistics after a stop_simulation() call.
If running an OS-based simulation, RTEMS will automatically issue a stop_simulation()
call at the end of its shutdown, so you don't need to use this call in OS-based
benchmarks.

The testbench can choose what to do. The most typical choices are putting a
stop_metric() and stop_simulation() calls at the beginning of the main routine, to
monitor OS boot (remembering to set the -a switch on in the simulator),
or putting explicit start_metric(), stop_metric() and stop_simulation() calls in
strategic positions of the code.

The dump_metric() call might be useful in some situations. This call performs
a dump of the current statistics of just the invoking processor. This dump should
usually be quite accurate, though it will be cycle-accurate only if following a
stop_metric() call. The report is appended to the same stats.txt file to which
global statistics will be written at the end of the simulation, so you are
advised to use this call with care to keep outputs manageable. dump_metric()
has no effect on access trace collection.

For non-RTEMS benchmarks, an useful pr() call is also provided to replace the
missing I/O libraries. Output will be directly sent to the console from which
the simulation was started. Please refer to the provided samples for additional
information. If you have access to the source code of the simulator, you may
want to edit the $SWARMDIR/arm/user_swi.cpp source file to add your own
custom messages: some free slots are provided for this purpose. pr() has no
effect on access trace collection.
